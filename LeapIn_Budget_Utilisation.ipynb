{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IFN711 - Budget Analysis for Leap in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install the dependencies stored in *requirments.txt*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\r\n",
    "import pymysql\r\n",
    "import os\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import seaborn as sns\r\n",
    "from apyori import apriori\r\n",
    "# Imports for the neural network\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score, roc_curve\r\n",
    "from sklearn.model_selection import GridSearchCV\r\n",
    "from sklearn.preprocessing import StandardScaler\r\n",
    "from sklearn.neural_network import MLPClassifier\r\n",
    "from sklearn.feature_selection import RFECV, SelectFromModel\r\n",
    "from sklearn.linear_model import LogisticRegression\r\n",
    "# Load the .env file for database credentials\r\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sign into the Database using credentials either stored in .env file, or **manual input**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare variables\r\n",
    "host = None\r\n",
    "port = None\r\n",
    "db = None\r\n",
    "user = None\r\n",
    "password = None\r\n",
    "# Load the .env file to access database credentials\r\n",
    "if load_dotenv():\r\n",
    "    # Assign required values to variables\r\n",
    "    host=os.getenv(\"host\")\r\n",
    "    port=int(os.getenv(\"port\"))\r\n",
    "    db=os.getenv(\"dbname\")\r\n",
    "    user=os.getenv(\"user\")\r\n",
    "    password=os.getenv(\"password\")\r\n",
    "    # Confirm loading of credentials\r\n",
    "    print(\"Database Credentials Loaded Successfully.\")\r\n",
    "# If unable to load env file, take manual input.\r\n",
    "else:\r\n",
    "    # Confirm failure of loading of credentials\r\n",
    "    print(\"Unable to detect Database Credentials. Please enter credentials manually.\\n\")\r\n",
    "    # Request manual entry of credentials\r\n",
    "    host = input(\"\\nPlease enter host address: \")\r\n",
    "    user = input(\"\\nPlease enter username: \")\r\n",
    "    password = input(\"\\nPlease enter password: \")\r\n",
    "    port = input(\"\\nPlease enter port number: \")\r\n",
    "    db = input(\"\\nPlease enter database name: \")\r\n",
    "    # Convert port to int\r\n",
    "    port = int(port)\r\n",
    "####### Connection to Client Database #######\r\n",
    "conn = pymysql.connect(host=host, user=user, port=port, password=password, db=db)\r\n",
    "# Print connection confirmation\r\n",
    "print(conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a dataframe of customer/user information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user= pd.read_sql(\"select m.id as member_id, m.membership_number, m.status, m.price_zone_code, m.member_key, s.u_ndis_number, s.u_disabilities, s.u_gender, s.u_date_of_birth, r.SA1, r.SA2, r.SA3,r.SA4  from SNOW_csm_consumer_user s left join  HH_member m  on s.u_ndis_number = m.membership_number left join libe_leapinprod_memberregion r on r.MemberId = s.u_leapin_id where s.u_stage = 'li_managed' and s.u_ndis_number is not null;\", con=conn)\r\n",
    "\r\n",
    "# Display summary information of df_user          \r\n",
    "df_user.info()\r\n",
    "df_user.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a dataframe of claims information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_claims= pd.read_sql(\"SELECT c.id as claim_id, c.invoice_id, c.state, c.risk_level, c.start_date FROM HH_claim c;\", con=conn)\r\n",
    "\r\n",
    "# Display summary information of df_user\r\n",
    "df_claims.info()\r\n",
    "df_claims.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Make a dataframe of Invoice information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_invoices= pd.read_sql(\"SELECT i.id as invoice_id, i.member_id, i.invoice_total, i.funded_total, i.funded_date FROM HH_invoice i;\", con=conn)\r\n",
    "\r\n",
    "# Display summary information of df_invoices          \r\n",
    "df_invoices.info()\r\n",
    "df_invoices.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge df_invoice and df_claims."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_invoice_claim = pd.merge(df_claims, df_invoices, on=\"invoice_id\", how=\"left\")\r\n",
    "\r\n",
    "# Display summary information of df_invoice_claim\r\n",
    "df_invoice_claim.info()\r\n",
    "df_invoice_claim.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge df_invoice_claim with df_user on member_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df_invoice_claim, df_user, on=\"member_id\", how=\"left\")\r\n",
    "\r\n",
    "# Summary information for df.\r\n",
    "df.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a copy of df for the clustering model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_cluster = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate invoice_total and funded_total values by summing them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.groupby([\"membership_number\"]).agg({'invoice_total': 'sum', 'funded_total': 'sum'}).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subtract the funded_total from invoice_total to see the extent to which each customer gets their reimbursements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a \"subtraction\" column\r\n",
    "df1['subtraction'] = df1['invoice_total'] - df1['funded_total']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary information of df1\r\n",
    "df1.info()\r\n",
    "df1.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge the modified invoice-claim df with the user df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.merge(df1, df_user, on=\"membership_number\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary information of df2.\r\n",
    "df2.info()\r\n",
    "df2.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a subset of df2 containing specific columns of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_of_interest = {'membership_number', 'invoice_total', 'funded_total', 'subtraction', 'u_disabilities', 'u_gender', 'price_zone_code', 'status'}\r\n",
    "\r\n",
    "df2 = df2[cols_of_interest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary information of df2.\r\n",
    "df2.info()\r\n",
    "df2.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning of df2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *u_disabilities*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Replace blank values with \"others\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['u_disabilities'] = df2['u_disabilities'].replace([''],'others')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Aggregate all non \"other\"-values to \"Intellectual\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['u_disabilities'].mask(df2['u_disabilities'] != 'others', \"Intellectual\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary information of disabilities.\r\n",
    "df2['u_disabilities'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REMOVE: Create two subsets of data based on the amounts reimbursed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Variables for threshold.\r\n",
    "# ! DO WE NEED THIS?\r\n",
    "sub_0_threshold = 1000\r\n",
    "sub_1_threshold = 10000\r\n",
    "\r\n",
    "# Create 2 subsets. \r\n",
    "sub_0 = df2[df2['subtraction']> sub_0_threshold]\r\n",
    "\r\n",
    "sub_1 = df2[df2['subtraction']> sub_1_threshold]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *u_gender* - Replace blank values with \"Unknown\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df2 = df2\r\n",
    "\r\n",
    "cleaned_df2[\"u_gender\"] = cleaned_df2[\"u_gender\"].replace([\"\"],\"Unknown\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *status* - Drop column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\r\n",
    "cleaned_df2 = cleaned_df2.drop(columns=[\"status\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *price_zone_code* - Drop column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df2 = cleaned_df2.drop(columns=[\"price_zone_code\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace *price_zone_code* with SA4 information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user_min = df_user[[\"membership_number\", \"SA4\"]]\r\n",
    "# Remove rows with blank SA1 - SA4 values\r\n",
    "blankIndices = df_user_min[df_user_min[\"SA4\"] == 0.0].index\r\n",
    "df_user_min = df_user_min.drop(axis=0, labels=blankIndices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with null SA1-4 values \n",
    "df_user_min = df_user_min.dropna()\n",
    "# Drop rows with duplicate membership_number values \n",
    "df_user_min = df_user_min.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain geographical locations using SA4 table dump.\r\n",
    "df_sa4 = pd.read_csv(\"./TableDump/SA4_2016.csv\",sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split values into columns.\r\n",
    "df_sa4[['SA4_CODE_2016','SA4_NAME_2016','GCCSA_CODE_2016','GCCSA_NAME_2016','STATE_CODE_2016','STATE_NAME_2016','AREA_ALBERS_SQKM']] = df_sa4['SA4_CODE_2016,SA4_NAME_2016,GCCSA_CODE_2016,GCCSA_NAME_2016,STATE_CODE_2016,STATE_NAME_2016,AREA_ALBERS_SQKM'].str.split(',',expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract relevant columns\n",
    "df_sa4 = df_sa4[[\"SA4_CODE_2016\", \"SA4_NAME_2016\", \"GCCSA_NAME_2016\", \"STATE_NAME_2016\"]]\n",
    "# Rename columns to match df2\n",
    "name_mapping = {\n",
    "    \"SA4_CODE_2016\": \"SA4\",\n",
    "    \"SA4_NAME_2016\": \"SA4_NAME\",\n",
    "    \"GCCSA_NAME_2016\": \"GCCSA_NAME\",\n",
    "    \"STATE_NAME_2016\": \"STATE_NAME\"\n",
    "}\n",
    "df_sa4 = df_sa4.rename(columns=name_mapping)\n",
    "df_sa4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set float for SA4:\r\n",
    "df_sa4['SA4'] = df_sa4['SA4'].astype(float, errors = 'raise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary information of df_sa4\r\n",
    "df_sa4.info()\r\n",
    "df_sa4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge SA4 information with the user DF.\r\n",
    "df_user_min = pd.merge(df_user_min, df_sa4, on= \"SA4\", how=\"left\")\r\n",
    "# Merge the final user info with DF2\r\n",
    "cleaned_df2 = pd.merge(cleaned_df2, df_user_min, on=\"membership_number\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add the *member_key* column to facilitate invoice-related calculations.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df2 = pd.merge(cleaned_df2, df_user[[\"membership_number\", \"member_key\"]], on=\"membership_number\", how=\"left\")\r\n",
    "# Drop duplicated member keys\r\n",
    "cleaned_df2 = cleaned_df2.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary information for cleaned_df2.\r\n",
    "cleaned_df2.info()\r\n",
    "cleaned_df2.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get information on Plans with status of \"completed\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Completed Plans\r\n",
    "df_plan = pd.read_sql(\"select p.plan_key, p.member_key, p.status, p.start_date, p.end_date, pb.item_category_level2_key, pb.allocation, pb.remaining from HH_plan p join HH_plan_budget pb on p.plan_key = pb.plan_key where p.status = 'COMPLETED'\", con=conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert start and end_date to datetime\r\n",
    "df_plan[\"start_date\"] = pd.to_datetime(df_plan[\"start_date\"], format=\"%Y-%m-%d\")\r\n",
    "df_plan[\"end_date\"] = pd.to_datetime(df_plan[\"end_date\"], format=\"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary info for df_plan\r\n",
    "df_plan.info()\r\n",
    "df_plan.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plan2 = df_plan\r\n",
    "## CHECK IF THIS IS NEEDED.\r\n",
    "# # Convert start and end_date to datetime\r\n",
    "# df_plan2[\"start_date\"] = pd.to_datetime(df_plan2[\"start_date\"], format=\"%Y-%m-%d\")\r\n",
    "# df_plan2[\"end_date\"] = pd.to_datetime(df_plan2[\"end_date\"], format=\"%Y-%m-%d\")\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group plans by *plan_key*, and then aggregate values as necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped_plan2 = df_plan2.groupby([\"plan_key\"]).agg({\"allocation\": \"sum\", \"remaining\": \"sum\", \"member_key\": \"first\", \"start_date\": \"first\", \"end_date\": \"first\"}).reset_index()\r\n",
    "\r\n",
    "# Sort grouped df by start and end dates in descending order\r\n",
    "df_grouped_plan2 = df_grouped_plan2.sort_values([\"start_date\", \"end_date\"], ascending=[False, False])\r\n",
    "\r\n",
    "# Eliminate duplicate member keys by dropping all rows but the most recent ones\r\n",
    "df_grouped_plan2 = df_grouped_plan2.groupby([\"member_key\"]).agg({\"start_date\": \"first\", \"end_date\": \"first\", \"plan_key\": \"first\", \"allocation\": \"first\", \"remaining\": \"first\"}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary information for df_grouped_plan2\r\n",
    "df_grouped_plan2.info()\r\n",
    "df_grouped_plan2.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge plan information with cleaned_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df2 = pd.merge(cleaned_df2, df_grouped_plan2, on=\"member_key\", how=\"left\")\r\n",
    "\r\n",
    "# Drop members without completed plans\r\n",
    "temp_df2 = temp_df2.dropna(subset=[\"plan_key\"])\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the ratio of money spent to money allocated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df2[\"spending_ratio\"] = (temp_df2[\"allocation\"] - temp_df2[\"remaining\"]) / temp_df2[\"allocation\"]\r\n",
    "\r\n",
    "temp_df2[\"spent\"] = temp_df2[\"allocation\"] - temp_df2[\"remaining\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define columns *under_spent*, *over_spent* and *par_spent* in accordance with the defined thresholds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables defining under and appropriate spending ratio thresholds.                                                      \r\n",
    "under_spend_thres = 0.75\r\n",
    "par_spend_thres = 1.0\r\n",
    "\r\n",
    "temp_df2[\"under_spent\"] = temp_df2[\"spending_ratio\"] <= under_spend_thres\r\n",
    "temp_df2[\"over_spent\"] = temp_df2[\"spending_ratio\"] > par_spend_thres\r\n",
    "temp_df2[\"par_spent\"] = temp_df2[\"spending_ratio\"] == par_spend_thres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop outdated columns\r\n",
    "temp_df2 = temp_df2.drop(columns=[\"invoice_total\", \"funded_total\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary information for temp_df2\r\n",
    "temp_df2.info()\r\n",
    "temp_df2.head(100)\r\n",
    "temp_df2[\"under_spent\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-Hot Encoding of DF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_df2 = temp_df2.set_index(\"membership_number\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encoding of *u_gender*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_gender = pd.get_dummies(one_hot_df2[\"u_gender\"], prefix=\"gender\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with the cleaned DF2\r\n",
    "one_hot_df2 = pd.merge(one_hot_df2, one_hot_gender, on=\"membership_number\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encoding of *GCCSA_NAME*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile list of Greater regions\r\n",
    "greaterRegions = one_hot_df2[one_hot_df2[\"GCCSA_NAME\"].str.contains(\"Greater\", na=False)][\"GCCSA_NAME\"].value_counts().index.to_list()\r\n",
    "\r\n",
    "# Compile list of \"Rest of...\" regions\r\n",
    "restOfRegions = one_hot_df2[one_hot_df2[\"GCCSA_NAME\"].str.contains(\"Rest of\", na=False)][\"GCCSA_NAME\"].value_counts().index.to_list()\r\n",
    "\r\n",
    "# Compile list of \"Capital\" regions\r\n",
    "capitalRegions = one_hot_df2[one_hot_df2[\"GCCSA_NAME\"].str.contains(\"Capital\", na=False)][\"GCCSA_NAME\"].value_counts().index.to_list()\r\n",
    "\r\n",
    "# Replace \"Greater\" values with Urban\r\n",
    "for region in greaterRegions:\r\n",
    "    one_hot_df2 = one_hot_df2.replace([region], \"Urban\")\r\n",
    "\r\n",
    "# Replace \"Rest of...\" values with Rural\r\n",
    "for region in restOfRegions:\r\n",
    "    one_hot_df2 = one_hot_df2.replace([region], \"Rural\")\r\n",
    "    \r\n",
    "# Replace \"Capital\" regions with Urban\r\n",
    "for region in capitalRegions:\r\n",
    "    one_hot_df2 = one_hot_df2.replace([region], \"Urban\")\r\n",
    "    \r\n",
    "# Replace NaN values with \"Unknown\"\r\n",
    "one_hot_df2[\"GCCSA_NAME\"] = one_hot_df2[\"GCCSA_NAME\"].replace(np.nan, \"Unknown\")\r\n",
    "\r\n",
    "# Perform One-hot Encoding on GCCSA_NAME\r\n",
    "one_hot_region = pd.get_dummies(one_hot_df2[\"GCCSA_NAME\"], prefix=\"GCCSA\")\r\n",
    "one_hot_df2 = pd.merge(one_hot_df2, one_hot_region, on=\"membership_number\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert any *UINT8*-formatted columns to *Bool*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = one_hot_df2.select_dtypes(include=[np.uint8]).columns\r\n",
    "one_hot_df2[column_names] = one_hot_df2[column_names].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\r\n",
    "one_hot_df2 = one_hot_df2.drop(columns=[\"SA4\", \"SA4_NAME\", \"GCCSA_NAME\", \"STATE_NAME\", \"u_gender\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Summary information for one_hot_df2\r\n",
    "one_hot_df2.info()\r\n",
    "one_hot_df2.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot Encoding of *subtraction*\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assumed threshold for determining over/under-spending\r\n",
    "funded_threshold = 500\r\n",
    "\r\n",
    "df = one_hot_df2\r\n",
    "\r\n",
    "df['subtraction'] = df['subtraction'].astype(int)\r\n",
    "df['not_fully_funded'] = df['subtraction'].ge(funded_threshold)\r\n",
    "df['acceptable_funded'] = df['subtraction'].lt(funded_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot Encoding of *u_disabilities*\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, prefix=['u_dis'], columns=['u_disabilities'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model - Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre processing for clustering\r\n",
    "df_og = df_for_cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the average number of days from start_date to funded_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_og['start_date'] = pd.to_datetime(df_og['start_date'])\r\n",
    "df_og['funded_date'] = pd.to_datetime(df_og['funded_date'])\r\n",
    "df_og['days_between_start_funded'] = (df_og['funded_date'] - df_og['start_date']).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group by member_id \r\n",
    "df_clus = df_og.groupby([\"membership_number\"]).agg({'days_between_start_funded': 'mean'}).reset_index()\r\n",
    "# Drop days_between_start_funded\r\n",
    "df_clus = df_clus.dropna(subset=['days_between_start_funded'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary information for df_clus\r\n",
    "df_clus.info()\r\n",
    "df_clus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some Stuff here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df1 = pd.merge(temp_df2, df_clus, on=\"membership_number\", how=\"left\")\r\n",
    "final_df1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model - Association Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model - Neural Network\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop columns with unique values.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['spending_ratio',\"plan_key\", \"member_key\",\"start_date\", \"end_date\", 'subtraction', 'allocation', 'remaining', 'spent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary information for df\r\n",
    "df.info()\r\n",
    "df.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for generating the Neural Network Model.\r\n",
    "\r\n",
    "### *Uncomment this code if you wish to train the model again or if there are any changes in the dataset.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\r\n",
    "# Specify the target column for analysis\r\n",
    "target_column = \"under_spent\"\r\n",
    "# Create the input df\r\n",
    "input_df = df.drop([target_column], axis=1)\r\n",
    "# Create a target df\r\n",
    "target_df = df[target_column]\r\n",
    "# Set random state\r\n",
    "random_state = 10\r\n",
    "# Set test size\r\n",
    "test_size = 0.3\r\n",
    "# Nump-ify input_df2\r\n",
    "input_df_mat = input_df.to_numpy()\r\n",
    "# Split training and test data\r\n",
    "input_df_train, input_df_test, target_df_train, target_df_test = train_test_split(input_df_mat, target_df, test_size=test_size, stratify=target_df, random_state=random_state)\r\n",
    "# Get standard scaler\r\n",
    "scaler = StandardScaler()\r\n",
    "# Transform training and test data\r\n",
    "input_df_train = scaler.fit_transform(input_df_train, target_df_train)\r\n",
    "input_df_test = scaler.transform(input_df_test)\r\n",
    "# Generate a prediction\r\n",
    "model_1 = MLPClassifier(random_state=random_state)\r\n",
    "model_1.fit(input_df_train, target_df_train)\r\n",
    "target_prediction = model_1.predict(input_df_test)\r\n",
    "'''\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary Information of the model.\r\n",
    "print(\"Classification Report: \\n\", classification_report(target_df_test, target_prediction))\r\n",
    "print(\"Train Accuracy: \", model_1.score(input_df_train, target_df_train))\r\n",
    "print(\"Test Accuracy: \", model_1.score(input_df_test, target_df_test))\r\n",
    "print(\"Default Model Characteristics: \", model_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPTIONAL - Neural Network tuned with GridSearchCV\r\n",
    "\r\n",
    "### *Uncomment this code if you wish to train the model again or if there are any changes in the dataset.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\r\n",
    "hiddenLayerSizes = [(2,), (3, ), (4, ), (5, ), (6, ), (7, )]\r\n",
    "\r\n",
    "alpha = [0.01, 0.001, 0.0001, 0.00001]\r\n",
    "\r\n",
    "params = {'hidden_layer_sizes': hiddenLayerSizes, 'alpha': alpha}\r\n",
    "\r\n",
    "model_2 = GridSearchCV(param_grid=params, estimator=MLPClassifier(random_state=random_state), cv=10, n_jobs=-1)\r\n",
    "\r\n",
    "model_2.fit(input_df_train, target_df_train)\r\n",
    "\r\n",
    "target_prediction = model_2.predict(input_df_test)\r\n",
    "\r\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary Information of the model.\r\n",
    "print(\"Classification Report: \\n\", classification_report(target_df_test, target_prediction))\r\n",
    "print(\"Train Accuracy: \", model_2.score(input_df_train, target_df_train))\r\n",
    "print(\"Test Accuracy: \", model_2.score(input_df_test, target_df_test))\r\n",
    "print(\"Tuned Model Characteristics: \", model_2)\r\n",
    "print(\"Best Parameters: \\n\", model_2.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the Neural Network Model as a *.sav* file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\r\n",
    "filename = './FinalisedModels/nn_model.sav'\r\n",
    "pickle.dump(model_1, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests for .sav file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model from disk.\r\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\r\n",
    "result = loaded_model.score(input_df_test, target_df_test)\r\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\r\n",
    "import seaborn as sn\r\n",
    "import pandas as pd\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "array = [[33,2,0,0,0,0,0,0,0,1,3], \r\n",
    "        [3,31,0,0,0,0,0,0,0,0,0], \r\n",
    "        [0,4,41,0,0,0,0,0,0,0,1], \r\n",
    "        [0,1,0,30,0,6,0,0,0,0,1], \r\n",
    "        [0,0,0,0,38,10,0,0,0,0,0], \r\n",
    "        [0,0,0,3,1,39,0,0,0,0,4], \r\n",
    "        [0,2,2,0,4,1,31,0,0,0,2],\r\n",
    "        [0,1,0,0,0,0,0,36,0,2,0], \r\n",
    "        [0,0,0,0,0,0,1,5,37,5,1], \r\n",
    "        [3,0,0,0,0,0,0,0,0,39,0], \r\n",
    "        [0,0,0,0,0,0,0,0,0,0,38]]\r\n",
    "df_cm = pd.DataFrame(array, index = [i for i in \"ABCDEFGHIJK\"], columns = [i for i in \"ABCDEFGHIJK\"])\r\n",
    "plt.figure(figsize = (10,7))\r\n",
    "sn.heatmap(df_cm, annot=True)\r\n",
    "\r\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "04599353301972a4177495fb87ba9328f666fdd8bb33e7641e5bb86b309132ff"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "metadata": {
   "interpreter": {
    "hash": "04599353301972a4177495fb87ba9328f666fdd8bb33e7641e5bb86b309132ff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}