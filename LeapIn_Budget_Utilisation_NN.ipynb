{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IFN711 - Budget Analysis for Leap in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following cell install the required dependencies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\r\n",
    "import pymysql\r\n",
    "import os\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import seaborn as sns\r\n",
    "from apyori import apriori\r\n",
    "# Imports for the neural network\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score, roc_curve\r\n",
    "from sklearn.model_selection import GridSearchCV\r\n",
    "from sklearn.preprocessing import StandardScaler\r\n",
    "from sklearn.neural_network import MLPClassifier\r\n",
    "from sklearn.feature_selection import RFECV, SelectFromModel\r\n",
    "from sklearn.linear_model import LogisticRegression\r\n",
    "# Load the .env file for database credentials\r\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sign into the Database using credentials either stored in .env file, or **manual input**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database Credentials Loaded Successfully.\n",
      "<pymysql.connections.Connection object at 0x00000181E7B709D0>\n"
     ]
    }
   ],
   "source": [
    "# Declare variables\r\n",
    "host = None\r\n",
    "port = None\r\n",
    "db = None\r\n",
    "user = None\r\n",
    "password = None\r\n",
    "# Load the .env file to access database credentials\r\n",
    "if load_dotenv():\r\n",
    "    # Assign required values to variables\r\n",
    "    host=os.getenv(\"host\")\r\n",
    "    port=int(os.getenv(\"port\"))\r\n",
    "    db=os.getenv(\"dbname\")\r\n",
    "    user=os.getenv(\"user\")\r\n",
    "    password=os.getenv(\"password\")\r\n",
    "    # Confirm loading of credentials\r\n",
    "    print(\"Database Credentials Loaded Successfully.\")\r\n",
    "# If unable to load env file, take manual input.\r\n",
    "else:\r\n",
    "    # Confirm failure of loading of credentials\r\n",
    "    print(\"Unable to detect Database Credentials. Please enter credentials manually.\\n\")\r\n",
    "    # Request manual entry of credentials\r\n",
    "    host = input(\"\\nPlease enter host address: \")\r\n",
    "    user = input(\"\\nPlease enter username: \")\r\n",
    "    password = input(\"\\nPlease enter password: \")\r\n",
    "    port = input(\"\\nPlease enter port number: \")\r\n",
    "    db = input(\"\\nPlease enter database name: \")\r\n",
    "    # Convert port to int\r\n",
    "    port = int(port)\r\n",
    "####### Connection to Client Database #######\r\n",
    "conn = pymysql.connect(host=host, user=user, port=port, password=password, db=db)\r\n",
    "# Print connection confirmation\r\n",
    "print(conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a dataframe of customer/user information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6241 entries, 0 to 6240\n",
      "Data columns (total 13 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   member_id          6115 non-null   float64\n",
      " 1   membership_number  6115 non-null   object \n",
      " 2   status             6115 non-null   object \n",
      " 3   price_zone_code    6115 non-null   object \n",
      " 4   member_key         6115 non-null   object \n",
      " 5   u_ndis_number      6241 non-null   object \n",
      " 6   u_disabilities     6241 non-null   object \n",
      " 7   u_gender           6241 non-null   object \n",
      " 8   u_date_of_birth    6239 non-null   object \n",
      " 9   SA1                6021 non-null   float64\n",
      " 10  SA2                6021 non-null   float64\n",
      " 11  SA3                6021 non-null   float64\n",
      " 12  SA4                6021 non-null   float64\n",
      "dtypes: float64(5), object(8)\n",
      "memory usage: 634.0+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>member_id</th>\n",
       "      <th>membership_number</th>\n",
       "      <th>status</th>\n",
       "      <th>price_zone_code</th>\n",
       "      <th>member_key</th>\n",
       "      <th>u_ndis_number</th>\n",
       "      <th>u_disabilities</th>\n",
       "      <th>u_gender</th>\n",
       "      <th>u_date_of_birth</th>\n",
       "      <th>SA1</th>\n",
       "      <th>SA2</th>\n",
       "      <th>SA3</th>\n",
       "      <th>SA4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1613.0</td>\n",
       "      <td>916644409</td>\n",
       "      <td>managed</td>\n",
       "      <td>ACT_NSW_QLD_VIC</td>\n",
       "      <td>d74bcd00-d205-11ea-b0d9-53acaecd0b82</td>\n",
       "      <td>916644409</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1922-11-30</td>\n",
       "      <td>3100614.0</td>\n",
       "      <td>31006.0</td>\n",
       "      <td>30101.0</td>\n",
       "      <td>301.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3132.0</td>\n",
       "      <td>262354568</td>\n",
       "      <td>managed</td>\n",
       "      <td>ACT_NSW_QLD_VIC</td>\n",
       "      <td>5d057910-dce6-11ea-812e-09cdf4382371</td>\n",
       "      <td>262354568</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2007-04-06</td>\n",
       "      <td>1144614.0</td>\n",
       "      <td>11446.0</td>\n",
       "      <td>12303.0</td>\n",
       "      <td>123.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1786.0</td>\n",
       "      <td>233367363</td>\n",
       "      <td>managed</td>\n",
       "      <td>ACT_NSW_QLD_VIC</td>\n",
       "      <td>222759f0-d212-11ea-8b42-db603b1b9020</td>\n",
       "      <td>233367363</td>\n",
       "      <td></td>\n",
       "      <td>Female</td>\n",
       "      <td>1954-03-26</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2562.0</td>\n",
       "      <td>981297214</td>\n",
       "      <td>managed</td>\n",
       "      <td>ACT_NSW_QLD_VIC</td>\n",
       "      <td>b0a9ca70-d50d-11ea-8224-836b70d82925</td>\n",
       "      <td>981297214</td>\n",
       "      <td></td>\n",
       "      <td>Female</td>\n",
       "      <td>1978-12-24</td>\n",
       "      <td>3138504.0</td>\n",
       "      <td>31385.0</td>\n",
       "      <td>31401.0</td>\n",
       "      <td>314.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1614.0</td>\n",
       "      <td>631304335</td>\n",
       "      <td>managed</td>\n",
       "      <td>ACT_NSW_QLD_VIC</td>\n",
       "      <td>d75087f0-d205-11ea-b0d9-53acaecd0b82</td>\n",
       "      <td>631304335</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1995-08-18</td>\n",
       "      <td>2142119.0</td>\n",
       "      <td>21421.0</td>\n",
       "      <td>21701.0</td>\n",
       "      <td>217.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>2792.0</td>\n",
       "      <td>266947556</td>\n",
       "      <td>managed</td>\n",
       "      <td>ACT_NSW_QLD_VIC</td>\n",
       "      <td>c2ce1270-d6d3-11ea-9cd1-f194648955a5</td>\n",
       "      <td>266947556</td>\n",
       "      <td>Hypothyroidism , Fibromyalgia , Thyroid , Deaf</td>\n",
       "      <td>Female</td>\n",
       "      <td>1932-04-07</td>\n",
       "      <td>1158802.0</td>\n",
       "      <td>11588.0</td>\n",
       "      <td>12504.0</td>\n",
       "      <td>125.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1964.0</td>\n",
       "      <td>723367568</td>\n",
       "      <td>managed</td>\n",
       "      <td>ACT_NSW_QLD_VIC</td>\n",
       "      <td>614c9420-d21b-11ea-b617-61a96cd5c136</td>\n",
       "      <td>723367568</td>\n",
       "      <td></td>\n",
       "      <td>Male</td>\n",
       "      <td>1978-09-21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>551.0</td>\n",
       "      <td>708296665</td>\n",
       "      <td>managed</td>\n",
       "      <td>ACT_NSW_QLD_VIC</td>\n",
       "      <td>c3e4aad0-c7e6-11ea-bd70-2bdba120f26c</td>\n",
       "      <td>708296665</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2001-09-15</td>\n",
       "      <td>2110528.0</td>\n",
       "      <td>21105.0</td>\n",
       "      <td>20601.0</td>\n",
       "      <td>206.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>2079.0</td>\n",
       "      <td>331401992</td>\n",
       "      <td>managed</td>\n",
       "      <td>ACT_NSW_QLD_VIC</td>\n",
       "      <td>a80f41a0-d220-11ea-b70a-d17e589b1829</td>\n",
       "      <td>331401992</td>\n",
       "      <td></td>\n",
       "      <td>Male</td>\n",
       "      <td>1940-04-30</td>\n",
       "      <td>3107107.0</td>\n",
       "      <td>31071.0</td>\n",
       "      <td>30304.0</td>\n",
       "      <td>303.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>3829.0</td>\n",
       "      <td>515269056</td>\n",
       "      <td>managed</td>\n",
       "      <td>ACT_NSW_QLD_VIC</td>\n",
       "      <td>f1ade000-e52b-11ea-aa27-0bababe260e0</td>\n",
       "      <td>515269056</td>\n",
       "      <td></td>\n",
       "      <td>Male</td>\n",
       "      <td>1982-07-07</td>\n",
       "      <td>3104527.0</td>\n",
       "      <td>31045.0</td>\n",
       "      <td>30204.0</td>\n",
       "      <td>302.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    member_id membership_number   status  price_zone_code  \\\n",
       "0      1613.0         916644409  managed  ACT_NSW_QLD_VIC   \n",
       "1      3132.0         262354568  managed  ACT_NSW_QLD_VIC   \n",
       "2      1786.0         233367363  managed  ACT_NSW_QLD_VIC   \n",
       "3      2562.0         981297214  managed  ACT_NSW_QLD_VIC   \n",
       "4      1614.0         631304335  managed  ACT_NSW_QLD_VIC   \n",
       "..        ...               ...      ...              ...   \n",
       "95     2792.0         266947556  managed  ACT_NSW_QLD_VIC   \n",
       "96     1964.0         723367568  managed  ACT_NSW_QLD_VIC   \n",
       "97      551.0         708296665  managed  ACT_NSW_QLD_VIC   \n",
       "98     2079.0         331401992  managed  ACT_NSW_QLD_VIC   \n",
       "99     3829.0         515269056  managed  ACT_NSW_QLD_VIC   \n",
       "\n",
       "                              member_key u_ndis_number  \\\n",
       "0   d74bcd00-d205-11ea-b0d9-53acaecd0b82     916644409   \n",
       "1   5d057910-dce6-11ea-812e-09cdf4382371     262354568   \n",
       "2   222759f0-d212-11ea-8b42-db603b1b9020     233367363   \n",
       "3   b0a9ca70-d50d-11ea-8224-836b70d82925     981297214   \n",
       "4   d75087f0-d205-11ea-b0d9-53acaecd0b82     631304335   \n",
       "..                                   ...           ...   \n",
       "95  c2ce1270-d6d3-11ea-9cd1-f194648955a5     266947556   \n",
       "96  614c9420-d21b-11ea-b617-61a96cd5c136     723367568   \n",
       "97  c3e4aad0-c7e6-11ea-bd70-2bdba120f26c     708296665   \n",
       "98  a80f41a0-d220-11ea-b70a-d17e589b1829     331401992   \n",
       "99  f1ade000-e52b-11ea-aa27-0bababe260e0     515269056   \n",
       "\n",
       "                                    u_disabilities u_gender u_date_of_birth  \\\n",
       "0                                                                1922-11-30   \n",
       "1                                                                2007-04-06   \n",
       "2                                                    Female      1954-03-26   \n",
       "3                                                    Female      1978-12-24   \n",
       "4                                                                1995-08-18   \n",
       "..                                             ...      ...             ...   \n",
       "95  Hypothyroidism , Fibromyalgia , Thyroid , Deaf   Female      1932-04-07   \n",
       "96                                                     Male      1978-09-21   \n",
       "97                                                               2001-09-15   \n",
       "98                                                     Male      1940-04-30   \n",
       "99                                                     Male      1982-07-07   \n",
       "\n",
       "          SA1      SA2      SA3    SA4  \n",
       "0   3100614.0  31006.0  30101.0  301.0  \n",
       "1   1144614.0  11446.0  12303.0  123.0  \n",
       "2         0.0      0.0      0.0    0.0  \n",
       "3   3138504.0  31385.0  31401.0  314.0  \n",
       "4   2142119.0  21421.0  21701.0  217.0  \n",
       "..        ...      ...      ...    ...  \n",
       "95  1158802.0  11588.0  12504.0  125.0  \n",
       "96        0.0      0.0      0.0    0.0  \n",
       "97  2110528.0  21105.0  20601.0  206.0  \n",
       "98  3107107.0  31071.0  30304.0  303.0  \n",
       "99  3104527.0  31045.0  30204.0  302.0  \n",
       "\n",
       "[100 rows x 13 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_user= pd.read_sql(\"select m.id as member_id, m.membership_number, m.status, m.price_zone_code, m.member_key, s.u_ndis_number, s.u_disabilities, s.u_gender, s.u_date_of_birth, r.SA1, r.SA2, r.SA3,r.SA4  from SNOW_csm_consumer_user s left join  HH_member m  on s.u_ndis_number = m.membership_number left join libe_leapinprod_memberregion r on r.MemberId = s.u_leapin_id where s.u_stage = 'li_managed' and s.u_ndis_number is not null;\", con=conn)\r\n",
    "\r\n",
    "# Display summary information of df_user          \r\n",
    "df_user.info()\r\n",
    "df_user.head(100)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_user['u_disabilities'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a dataframe of claims information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_claims= pd.read_sql(\"SELECT c.id as claim_id, c.invoice_id, c.state, c.risk_level, c.start_date FROM HH_claim c;\", con=conn)\r\n",
    "\r\n",
    "# Display summary information of df_user\r\n",
    "df_claims.info()\r\n",
    "df_claims.head(100)\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Make a dataframe of Invoice information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_invoices= pd.read_sql(\"SELECT i.id as invoice_id, i.member_id, i.invoice_total, i.funded_total, i.funded_date FROM HH_invoice i;\", con=conn)\r\n",
    "\r\n",
    "# Display summary information of df_invoices          \r\n",
    "df_invoices.info()\r\n",
    "df_invoices.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge df_invoice and df_claims."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_invoice_claim = pd.merge(df_claims, df_invoices, on=\"invoice_id\", how=\"left\")\r\n",
    "\r\n",
    "# Display summary information of df_invoice_claim\r\n",
    "df_invoice_claim.info()\r\n",
    "df_invoice_claim.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\r\n",
    "# Mapping users' disability type:\r\n",
    "\r\n",
    "df = pd.merge(df_invoice_claim, df_user, on=\"member_id\", how=\"left\")\r\n",
    "\r\n",
    "# Make a copy for the clustering model. \r\n",
    "df_for_cluster = df.copy()\r\n",
    "\r\n",
    "# Summary information for df \r\n",
    "df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Groupby member_id \n",
    "\n",
    "df1 = df.groupby([\"membership_number\"]).agg({'invoice_total': 'sum', 'funded_total': 'sum'}).reset_index()\n",
    "\n",
    "df1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subtract the invoice_total to fudned_total to see how user get reimbursemente - creat subtraction column\n",
    "\n",
    "df1['subtraction'] = df1['invoice_total'] - df1['funded_total']\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging again\n",
    "\n",
    "df2 = pd.merge(df1, df_user, on=\"membership_number\", how=\"left\")\n",
    "df2.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select interested columns\n",
    "\n",
    "cols_of_interest = {'membership_number', 'invoice_total', 'funded_total', 'subtraction', 'status', 'price_zone_code', \n",
    "                   'u_disabilities', 'u_gender'}\n",
    "df2 = df2[cols_of_interest]\n",
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Re-arrange that mess\n",
    "\n",
    "df2 = df2[['membership_number', 'invoice_total', 'funded_total', 'subtraction',   \n",
    "                   'u_disabilities', 'u_gender','price_zone_code','status']]\n",
    "\n",
    "df2.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking u_dis\n",
    "\n",
    "df2['u_disabilities'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace blank value with 'others'\n",
    "\n",
    "df2['u_disabilities'] = df2['u_disabilities'].replace([''],'others')\n",
    "\n",
    "df2['u_disabilities'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate all categories != others into Intellectual\n",
    "\n",
    "df2['u_disabilities'].mask(df2['u_disabilities'] != 'others', \"Intellectual\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['u_disabilities'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Exploring to have a clearer view on the overall of reimbursement:\n",
    "\n",
    "sub_0 = df2[df2['subtraction']> 1000]\n",
    "#sub_0\n",
    "\n",
    "sub_1 = df2[df2['subtraction']> 10000]\n",
    "sub_1\n",
    "\n",
    "#Set value for rows matching condition\n",
    "\n",
    "#df2[['subtraction'] > 1000] == 'High'\n",
    "#df2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modifications to df2\n",
    "cleaned_df2 = df2\n",
    "\n",
    "# gender\t\t: Add \"other\" for blank or null values --> Male/Female/Other/Unknown\n",
    "cleaned_df2[\"u_gender\"] = cleaned_df2[\"u_gender\"].replace([\"\"],\"Unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# status\t\t: drop\n",
    "cleaned_df2 = cleaned_df2.drop(columns=[\"status\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# price_code\t: drop, replace with SA4 and its information\n",
    "cleaned_df2 = cleaned_df2.drop(columns=[\"price_zone_code\"])\n",
    "df_user_min = df_user[[\"membership_number\", \"SA4\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with blank SA1 - SA4 values\n",
    "blankIndices = df_user_min[df_user_min[\"SA4\"] == 0.0].index\n",
    "df_user_min = df_user_min.drop(axis=0, labels=blankIndices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with null SA1-4 values \n",
    "df_user_min = df_user_min.dropna()\n",
    "# Drop rows with duplicate membership_number values \n",
    "df_user_min = df_user_min.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain geographical locations using SA4\n",
    "df_sa4 = pd.read_csv(\"./TableDump/SA4_2016.csv\",sep='\\t')\n",
    "df_sa4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split values into columns\n",
    "\n",
    "df_sa4[['SA4_CODE_2016','SA4_NAME_2016','GCCSA_CODE_2016','GCCSA_NAME_2016','STATE_CODE_2016','STATE_NAME_2016','AREA_ALBERS_SQKM']] = df_sa4['SA4_CODE_2016,SA4_NAME_2016,GCCSA_CODE_2016,GCCSA_NAME_2016,STATE_CODE_2016,STATE_NAME_2016,AREA_ALBERS_SQKM'].str.split(',',expand=True)\n",
    "df_sa4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract relevant columns\n",
    "df_sa4 = df_sa4[[\"SA4_CODE_2016\", \"SA4_NAME_2016\", \"GCCSA_NAME_2016\", \"STATE_NAME_2016\"]]\n",
    "# Rename columns to match df2\n",
    "name_mapping = {\n",
    "    \"SA4_CODE_2016\": \"SA4\",\n",
    "    \"SA4_NAME_2016\": \"SA4_NAME\",\n",
    "    \"GCCSA_NAME_2016\": \"GCCSA_NAME\",\n",
    "    \"STATE_NAME_2016\": \"STATE_NAME\"\n",
    "}\n",
    "df_sa4 = df_sa4.rename(columns=name_mapping)\n",
    "df_sa4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set float for SA4:\n",
    "df_sa4['SA4'] = df_sa4['SA4'].astype(float, errors = 'raise')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge SA4 information with the user DF\n",
    "df_user_min = pd.merge(df_user_min, df_sa4, on= \"SA4\", how=\"left\")\n",
    "# Merge the final user info with DF2\n",
    "cleaned_df2 = pd.merge(cleaned_df2, df_user_min, on=\"membership_number\", how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Member_key to deal with invoice calculations\n",
    "cleaned_df2 = pd.merge(cleaned_df2, df_user[[\"membership_number\", \"member_key\"]], on=\"membership_number\", how=\"left\")\n",
    "# Drop duplicated member keys\n",
    "cleaned_df2 = cleaned_df2.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Summary information for cleaned_df2\n",
    "cleaned_df2.info()\n",
    "cleaned_df2.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Completed Plans\n",
    "df_plan = pd.read_sql(\"select p.plan_key, p.member_key, p.status, p.start_date, p.end_date, pb.item_category_level2_key, pb.allocation, pb.remaining from HH_plan p join HH_plan_budget pb on p.plan_key = pb.plan_key where p.status = 'COMPLETED'\", con=conn)\n",
    "# Convert start and end_date to datetime\n",
    "df_plan[\"start_date\"] = pd.to_datetime(df_plan[\"start_date\"], format=\"%Y-%m-%d\")\n",
    "df_plan[\"end_date\"] = pd.to_datetime(df_plan[\"end_date\"], format=\"%Y-%m-%d\")\n",
    "# Summary info for df_plan\n",
    "df_plan.info()\n",
    "df_plan.head(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plan2 = df_plan\n",
    "\n",
    "# Convert start and end_date to datetime\n",
    "df_plan2[\"start_date\"] = pd.to_datetime(df_plan2[\"start_date\"], format=\"%Y-%m-%d\")\n",
    "df_plan2[\"end_date\"] = pd.to_datetime(df_plan2[\"end_date\"], format=\"%Y-%m-%d\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum up all based on plan_key\n",
    "\n",
    "df_grouped_plan2 = df_plan2.groupby([\"plan_key\"]).agg({\"allocation\": \"sum\", \"remaining\": \"sum\", \"member_key\": \"first\", \"start_date\": \"first\", \"end_date\": \"first\"}).reset_index()\n",
    "\n",
    "# Sort grouped df by start and end dates in descending order\n",
    "\n",
    "df_grouped_plan2 = df_grouped_plan2.sort_values([\"start_date\", \"end_date\"], ascending=[False, False])\n",
    "\n",
    "# Eliminate duplicate member keys by dropping all rows but the most recent ones\n",
    "\n",
    "df_grouped_plan2 = df_grouped_plan2.groupby([\"member_key\"]).agg({\"start_date\": \"first\", \"end_date\": \"first\", \"plan_key\": \"first\", \"allocation\": \"first\", \"remaining\": \"first\"}).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary information for df_plan2\n",
    "\n",
    "df_grouped_plan2.info()\n",
    "df_grouped_plan2.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge plan information with cleaned_df2\n",
    "\n",
    "temp_df2 = pd.merge(cleaned_df2, df_grouped_plan2, on=\"member_key\", how=\"left\")\n",
    "\n",
    "# Drop members without completed plans\n",
    "\n",
    "temp_df2 = temp_df2.dropna(subset=[\"plan_key\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate ratio of spent-allocated\r\n",
    "\r\n",
    "temp_df2[\"spending_ratio\"] = (temp_df2[\"allocation\"] - temp_df2[\"remaining\"]) / temp_df2[\"allocation\"]\r\n",
    "temp_df2[\"spent\"] = temp_df2[\"allocation\"] - temp_df2[\"remaining\"]\r\n",
    "\r\n",
    "# Variables defining under and appropriate spending ratio thresholds.\r\n",
    "                                                      \r\n",
    "under_spend_thres = 0.75\r\n",
    "par_spend_thres = 1.0\r\n",
    "\r\n",
    "temp_df2[\"under_spent\"] = temp_df2[\"spending_ratio\"] <= under_spend_thres\r\n",
    "temp_df2[\"over_spent\"] = temp_df2[\"spending_ratio\"] > par_spend_thres\r\n",
    "temp_df2[\"par_spent\"] = temp_df2[\"spending_ratio\"] == par_spend_thres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop outdated columns\n",
    "\n",
    "temp_df2 = temp_df2.drop(columns=[\"invoice_total\", \"funded_total\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Summary information for temp_df2\n",
    "\n",
    "temp_df2.info()\n",
    "temp_df2.head(100)\n",
    "temp_df2[\"under_spent\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot Encoding of temp_df2\n",
    "\n",
    "one_hot_df2 = temp_df2.set_index(\"membership_number\")\n",
    "\n",
    "# One-hot encoding of Gender\n",
    "\n",
    "one_hot_gender = pd.get_dummies(one_hot_df2[\"u_gender\"], prefix=\"gender\")\n",
    "\n",
    "# Merge with the cleaned DF2\n",
    "\n",
    "one_hot_df2 = pd.merge(one_hot_df2, one_hot_gender, on=\"membership_number\", how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding of GCCSA\n",
    "\n",
    "# Compile list of Greater regions\n",
    "\n",
    "greaterRegions = one_hot_df2[one_hot_df2[\"GCCSA_NAME\"].str.contains(\"Greater\", na=False)][\"GCCSA_NAME\"].value_counts().index.to_list()\n",
    "\n",
    "# Compile list of \"Rest of...\" regions\n",
    "\n",
    "restOfRegions = one_hot_df2[one_hot_df2[\"GCCSA_NAME\"].str.contains(\"Rest of\", na=False)][\"GCCSA_NAME\"].value_counts().index.to_list()\n",
    "\n",
    "# Compile list of \"Capital\" regions\n",
    "\n",
    "capitalRegions = one_hot_df2[one_hot_df2[\"GCCSA_NAME\"].str.contains(\"Capital\", na=False)][\"GCCSA_NAME\"].value_counts().index.to_list()\n",
    "\n",
    "# Replace \"Greater\" values with Urban\n",
    "\n",
    "for region in greaterRegions:\n",
    "    one_hot_df2 = one_hot_df2.replace([region], \"Urban\")\n",
    "\n",
    "# Replace \"Rest of...\" values with Rural\n",
    "\n",
    "for region in restOfRegions:\n",
    "    one_hot_df2 = one_hot_df2.replace([region], \"Rural\")\n",
    "    \n",
    "# Replace \"Capital\" regions with Urban\n",
    "\n",
    "for region in capitalRegions:\n",
    "    one_hot_df2 = one_hot_df2.replace([region], \"Urban\")\n",
    "    \n",
    "# Replace NaN values with \"Unknown\"\n",
    "\n",
    "one_hot_df2[\"GCCSA_NAME\"] = one_hot_df2[\"GCCSA_NAME\"].replace(np.nan, \"Unknown\")\n",
    "\n",
    "# Perform One-hot Encoding on GCCSA_NAME\n",
    "\n",
    "one_hot_region = pd.get_dummies(one_hot_df2[\"GCCSA_NAME\"], prefix=\"GCCSA\")\n",
    "one_hot_df2 = pd.merge(one_hot_df2, one_hot_region, on=\"membership_number\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert any UINT8 columns to Bool\n",
    "\n",
    "column_names = one_hot_df2.select_dtypes(include=[np.uint8]).columns\n",
    "one_hot_df2[column_names] = one_hot_df2[column_names].astype(bool)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "\n",
    "one_hot_df2 = one_hot_df2.drop(columns=[\"SA4\", \"SA4_NAME\", \"GCCSA_NAME\", \"STATE_NAME\", \"u_gender\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Summary information for one_hot_df2\n",
    "\n",
    "one_hot_df2.info()\n",
    "one_hot_df2.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual one-hot encode subtraction:\n",
    "# Assumed threshold for determining over/under-spending\n",
    "funded_threshold = 500\n",
    "df = one_hot_df2\n",
    "df['subtraction'] = df['subtraction'].astype(int)\n",
    "df['not_fully_funded'] = df['subtraction'].ge(funded_threshold)\n",
    "df['acceptable_funded'] = df['subtraction'].lt(funded_threshold)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One hot encode u_disabilities\n",
    "df = pd.get_dummies(df, prefix=['u_dis'], columns=['u_disabilities'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL 1 - CLUSTERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre processing for clustering\r\n",
    "df_og = df_for_cluster\r\n",
    "df_og.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for getting the avg of days from start to end\r\n",
    "df_og['start_date'] = pd.to_datetime(df_og['start_date'])\r\n",
    "df_og['funded_date'] = pd.to_datetime(df_og['funded_date'])\r\n",
    "df_og['days_between_start_funded'] = (df_og['funded_date'] - df_og['start_date']).dt.days\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Groupby member_id \r\n",
    "\r\n",
    "df_clus = df_og.groupby([\"membership_number\"]).agg({'days_between_start_funded': 'mean'}).reset_index()\r\n",
    "\r\n",
    "df_clus = df_clus.dropna(subset=['days_between_start_funded'])\r\n",
    "df_clus.info()\r\n",
    "df_clus\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df1 = pd.merge(temp_df2, df_clus, on=\"membership_number\", how=\"left\")\r\n",
    "\r\n",
    "final_df1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL 2 - NEURAL NETWORK\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Drop columns with unique values.\n",
    "df = df.drop(columns=['spending_ratio',\"plan_key\", \"member_key\",\"start_date\", \"end_date\", 'subtraction', 'allocation', 'remaining', 'spent'])\n",
    "# Summary information for df\n",
    "df.info()\n",
    "df.head(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out model persistence where the trained model's weight was stored and loaded\n",
    "# To uncomment this code if you wish to train the model again or any changes subjected to the dataset\n",
    "\n",
    "# Specify the target column for analysis\n",
    "target_column = \"under_spent\"\n",
    "# Create the input df\n",
    "input_df = df.drop([target_column], axis=1)\n",
    "# Create a target df\n",
    "target_df = df[target_column]\n",
    "# Set random state\n",
    "random_state = 10\n",
    "# Set test size\n",
    "test_size = 0.3\n",
    "# Nump-ify input_df2\n",
    "input_df_mat = input_df.to_numpy()\n",
    "# Split training and test data\n",
    "input_df_train, input_df_test, target_df_train, target_df_test = train_test_split(input_df_mat, target_df, test_size=test_size, stratify=target_df, random_state=random_state)\n",
    "# Get standard scaler\n",
    "scaler = StandardScaler()\n",
    "# Transform training and test data\n",
    "input_df_train = scaler.fit_transform(input_df_train, target_df_train)\n",
    "input_df_test = scaler.transform(input_df_test)\n",
    "# Generate a prediction\n",
    "model_1 = MLPClassifier(random_state=random_state)\n",
    "model_1.fit(input_df_train, target_df_train)\n",
    "target_prediction = model_1.predict(input_df_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary Information\n",
    "print(\"Classification Report: \\n\", classification_report(target_df_test, target_prediction))\n",
    "print(\"Train Accuracy: \", model_1.score(input_df_train, target_df_train))\n",
    "print(\"Test Accuracy: \", model_1.score(input_df_test, target_df_test))\n",
    "print(\"Default Model Characteristics: \", model_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Improved Model - Neural Network tuned with GridSearchCV\n",
    "hiddenLayerSizes = [(2,), (3, ), (4, ), (5, ), (6, ), (7, )]\n",
    "\n",
    "alpha = [0.01, 0.001, 0.0001, 0.00001]\n",
    "\n",
    "params = {'hidden_layer_sizes': hiddenLayerSizes, 'alpha': alpha}\n",
    "\n",
    "model_2 = GridSearchCV(param_grid=params, estimator=MLPClassifier(random_state=random_state), cv=10, n_jobs=-1)\n",
    "\n",
    "model_2.fit(input_df_train, target_df_train)\n",
    "\n",
    "target_prediction = model_2.predict(input_df_test)\n",
    "# Summary Information\n",
    "print(\"Classification Report: \\n\", classification_report(target_df_test, target_prediction))\n",
    "print(\"Train Accuracy: \", model_2.score(input_df_train, target_df_train))\n",
    "print(\"Test Accuracy: \", model_2.score(input_df_test, target_df_test))\n",
    "print(\"Tuned Model Characteristics: \", model_2)\n",
    "print(\"Best Parameters: \\n\", model_2.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL PERSISTENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "#Save model weight:\n",
    "\n",
    "filename = 'finalized_model.sav'\n",
    "pickle.dump(model_1, open(filename, 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some time later...\n",
    " \n",
    "# load the model from disk\n",
    "\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "result = loaded_model.score(input_df_test, target_df_test)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "array = [[33,2,0,0,0,0,0,0,0,1,3], \n",
    "        [3,31,0,0,0,0,0,0,0,0,0], \n",
    "        [0,4,41,0,0,0,0,0,0,0,1], \n",
    "        [0,1,0,30,0,6,0,0,0,0,1], \n",
    "        [0,0,0,0,38,10,0,0,0,0,0], \n",
    "        [0,0,0,3,1,39,0,0,0,0,4], \n",
    "        [0,2,2,0,4,1,31,0,0,0,2],\n",
    "        [0,1,0,0,0,0,0,36,0,2,0], \n",
    "        [0,0,0,0,0,0,1,5,37,5,1], \n",
    "        [3,0,0,0,0,0,0,0,0,39,0], \n",
    "        [0,0,0,0,0,0,0,0,0,0,38]]\n",
    "df_cm = pd.DataFrame(array, index = [i for i in \"ABCDEFGHIJK\"],\n",
    "                  columns = [i for i in \"ABCDEFGHIJK\"])\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.heatmap(df_cm, annot=True)\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "04599353301972a4177495fb87ba9328f666fdd8bb33e7641e5bb86b309132ff"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "metadata": {
   "interpreter": {
    "hash": "04599353301972a4177495fb87ba9328f666fdd8bb33e7641e5bb86b309132ff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}