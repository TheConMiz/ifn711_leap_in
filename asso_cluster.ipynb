{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pymysql\n",
    "!pip install apyori\n",
    "!pip install pandas\n",
    "!pip install matplotlib\n",
    "!pip install seaborn\n",
    "!pip install scikit-learn\n",
    "!pip install python-dotenv\n",
    "!pip install pydot\n",
    "!pip install kmodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imported Libraries\n",
    "import pandas as pd\n",
    "import pymysql\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from apyori import apriori\n",
    "# Imports for the neural network\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_selection import RFECV, SelectFromModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n",
    "\n",
    "#Imports for Clustering\n",
    "from kmodes.kmodes import KModes\n",
    "from kmodes.kprototypes import KPrototypes\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "\n",
    "#######Connection to Client Database-- Needs Changes\n",
    "host=\"qut-ds.c2m09j1oykve.ap-southeast-2.rds.amazonaws.com\"\n",
    "port=4005\n",
    "dbname=\"qut_ds1\"\n",
    "user=\"qut_ds1\"\n",
    "password=\"GaAVSqC#9JR8\"\n",
    "conn = pymysql.connect(host=host, user=user,port=port,password=password, db=dbname)\n",
    "\n",
    "\n",
    "print(conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Customer-users detail - make dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Customer Personal details\n",
    "\n",
    "# ! Added Member key\n",
    "df_user= pd.read_sql(\"select m.id as member_id, m.membership_number, m.status, m.price_zone_code, m.member_key, s.u_ndis_number, s.u_disabilities, s.u_gender, s.u_date_of_birth, r.SA1, r.SA2, r.SA3,r.SA4  from SNOW_csm_consumer_user s left join  HH_member m  on s.u_ndis_number = m.membership_number left join libe_leapinprod_memberregion r on r.MemberId = s.u_leapin_id where s.u_stage = 'li_managed' and s.u_ndis_number is not null;\", con=conn)\n",
    "                       \n",
    "df_user.info()\n",
    "df_user.head(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user['u_disabilities'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Claims and Invoice details - make dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting Claims details: \n",
    "\n",
    "df_claims= pd.read_sql(\"SELECT c.id as claim_id, c.invoice_id, c.state, c.risk_level, c.start_date FROM HH_claim c;\", con=conn)\n",
    "df_claims.info()\n",
    "\n",
    "df_claims.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting Invoice Details\n",
    "\n",
    "df_invoices= pd.read_sql(\"SELECT i.id as invoice_id, i.member_id, i.invoice_total, i.funded_total, i.funded_date FROM HH_invoice i;\", con=conn)\n",
    "df_invoices.info()\n",
    "df_invoices.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging\n",
    "\n",
    "df_invoice_claim = pd.merge(df_claims, df_invoices, on=\"invoice_id\", how=\"left\")\n",
    "\n",
    "df_invoice_claim.info()\n",
    "df_invoice_claim.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Mapping users' disability type:\n",
    "\n",
    "df = pd.merge(df_invoice_claim, df_user, on=\"member_id\", how=\"left\")\n",
    "##Will be used separately for clustering\n",
    "df_for_cluster = df.copy()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Groupby member_id \n",
    "\n",
    "df1 = df.groupby([\"membership_number\"]).agg({'invoice_total': 'sum', 'funded_total': 'sum'}).reset_index()\n",
    "\n",
    "df1.info()\n",
    "df1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#subtract the invoice_total to fudned_total to see how user get reimbursemente - creat subtraction column\n",
    "\n",
    "df1['subtraction'] = df1['invoice_total'] - df1['funded_total']\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging again\n",
    "\n",
    "df2 = pd.merge(df1, df_user, on=\"membership_number\", how=\"left\")\n",
    "df2.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select interested columns\n",
    "\n",
    "cols_of_interest = {'membership_number', 'invoice_total', 'funded_total', 'subtraction', 'status', 'price_zone_code', \n",
    "                   'u_disabilities', 'u_gender'}\n",
    "df2 = df2[cols_of_interest]\n",
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Re-arrange that mess\n",
    "\n",
    "df2 = df2[['membership_number', 'invoice_total', 'funded_total', 'subtraction',   \n",
    "                   'u_disabilities', 'u_gender','price_zone_code','status']]\n",
    "\n",
    "df2.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking u_dis\n",
    "\n",
    "df2['u_disabilities'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace blank value with 'others'\n",
    "\n",
    "df2['u_disabilities'] = df2['u_disabilities'].replace([''],'others')\n",
    "\n",
    "df2['u_disabilities'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate all categories != others into Intellectual\n",
    "\n",
    "df2['u_disabilities'].mask(df2['u_disabilities'] != 'others', \"Intelectual\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['u_disabilities'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Exploring to have a clearer view on the overall of reimbursement:\n",
    "\n",
    "sub_0 = df2[df2['subtraction']> 1000]\n",
    "#sub_0\n",
    "\n",
    "sub_1 = df2[df2['subtraction']> 10000]\n",
    "sub_1\n",
    "\n",
    "#Set value for rows matching condition\n",
    "\n",
    "#df2[['subtraction'] > 1000] == 'High'\n",
    "#df2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modifications to df2\n",
    "cleaned_df2 = df2\n",
    "\n",
    "# gender\t\t: Add \"other\" for blank or null values --> Male/Female/Other/Unknown\n",
    "cleaned_df2[\"u_gender\"] = cleaned_df2[\"u_gender\"].replace([\"\"],\"Unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# status\t\t: drop\n",
    "cleaned_df2 = cleaned_df2.drop(columns=[\"status\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# price_code\t: drop, replace with SA4 and its information\n",
    "cleaned_df2 = cleaned_df2.drop(columns=[\"price_zone_code\"])\n",
    "df_user_min = df_user[[\"membership_number\", \"SA4\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with blank SA1 - SA4 values\n",
    "blankIndices = df_user_min[df_user_min[\"SA4\"] == 0.0].index\n",
    "df_user_min = df_user_min.drop(axis=0, labels=blankIndices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with null SA1-4 values \n",
    "df_user_min = df_user_min.dropna()\n",
    "# Drop rows with duplicate membership_number values \n",
    "df_user_min = df_user_min.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain geographical locations using SA4\n",
    "df_sa4 = pd.read_csv(\"./TableDump/SA4_2016.csv\",sep='\\t')\n",
    "df_sa4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split values into columns\n",
    "\n",
    "df_sa4[['SA4_CODE_2016','SA4_NAME_2016','GCCSA_CODE_2016','GCCSA_NAME_2016','STATE_CODE_2016','STATE_NAME_2016','AREA_ALBERS_SQKM']] = df_sa4['SA4_CODE_2016,SA4_NAME_2016,GCCSA_CODE_2016,GCCSA_NAME_2016,STATE_CODE_2016,STATE_NAME_2016,AREA_ALBERS_SQKM'].str.split(',',expand=True)\n",
    "df_sa4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract relevant columns\n",
    "df_sa4 = df_sa4[[\"SA4_CODE_2016\", \"SA4_NAME_2016\", \"GCCSA_NAME_2016\", \"STATE_NAME_2016\"]]\n",
    "# Rename columns to match df2\n",
    "name_mapping = {\n",
    "    \"SA4_CODE_2016\": \"SA4\",\n",
    "    \"SA4_NAME_2016\": \"SA4_NAME\",\n",
    "    \"GCCSA_NAME_2016\": \"GCCSA_NAME\",\n",
    "    \"STATE_NAME_2016\": \"STATE_NAME\"\n",
    "}\n",
    "df_sa4 = df_sa4.rename(columns=name_mapping)\n",
    "df_sa4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set float for SA4:\n",
    "df_sa4['SA4'] = df_sa4['SA4'].astype(float, errors = 'raise')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge SA4 information with the user DF\n",
    "df_user_min = pd.merge(df_user_min, df_sa4, on= \"SA4\", how=\"left\")\n",
    "# Merge the final user info with DF2\n",
    "cleaned_df2 = pd.merge(cleaned_df2, df_user_min, on=\"membership_number\", how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Member_key to deal with invoice calculations\n",
    "cleaned_df2 = pd.merge(cleaned_df2, df_user[[\"membership_number\", \"member_key\"]], on=\"membership_number\", how=\"left\")\n",
    "# Drop duplicated member keys\n",
    "cleaned_df2 = cleaned_df2.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Summary information for cleaned_df2\n",
    "cleaned_df2.info()\n",
    "cleaned_df2.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Completed Plans\n",
    "df_plan = pd.read_sql(\"select p.plan_key, p.member_key, p.status, p.start_date, p.end_date, pb.item_category_level2_key, pb.allocation, pb.remaining from HH_plan p join HH_plan_budget pb on p.plan_key = pb.plan_key where p.status = 'COMPLETED'\", con=conn)\n",
    "# Convert start and end_date to datetime\n",
    "df_plan[\"start_date\"] = pd.to_datetime(df_plan[\"start_date\"], format=\"%Y-%m-%d\")\n",
    "df_plan[\"end_date\"] = pd.to_datetime(df_plan[\"end_date\"], format=\"%Y-%m-%d\")\n",
    "# Summary info for df_plan\n",
    "df_plan.info()\n",
    "df_plan.head(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plan2 = df_plan\n",
    "\n",
    "# Convert start and end_date to datetime\n",
    "df_plan2[\"start_date\"] = pd.to_datetime(df_plan2[\"start_date\"], format=\"%Y-%m-%d\")\n",
    "df_plan2[\"end_date\"] = pd.to_datetime(df_plan2[\"end_date\"], format=\"%Y-%m-%d\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum up all based on plan_key\n",
    "\n",
    "df_grouped_plan2 = df_plan2.groupby([\"plan_key\"]).agg({\"allocation\": \"sum\", \"remaining\": \"sum\", \"member_key\": \"first\", \"start_date\": \"first\", \"end_date\": \"first\"}).reset_index()\n",
    "\n",
    "# Sort grouped df by start and end dates in descending order\n",
    "\n",
    "df_grouped_plan2 = df_grouped_plan2.sort_values([\"start_date\", \"end_date\"], ascending=[False, False])\n",
    "\n",
    "# Eliminate duplicate member keys by dropping all rows but the most recent ones\n",
    "\n",
    "df_grouped_plan2 = df_grouped_plan2.groupby([\"member_key\"]).agg({\"start_date\": \"first\", \"end_date\": \"first\", \"plan_key\": \"first\", \"allocation\": \"first\", \"remaining\": \"first\"}).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary information for df_plan2\n",
    "\n",
    "df_grouped_plan2.info()\n",
    "df_grouped_plan2.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge plan information with cleaned_df2\n",
    "\n",
    "temp_df2 = pd.merge(cleaned_df2, df_grouped_plan2, on=\"member_key\", how=\"left\")\n",
    "\n",
    "# Drop members without completed plans\n",
    "\n",
    "temp_df2 = temp_df2.dropna(subset=[\"plan_key\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate ratio of spent-allocated\n",
    "\n",
    "temp_df2[\"spending_ratio\"] = (temp_df2[\"allocation\"] - temp_df2[\"remaining\"]) / temp_df2[\"allocation\"]\n",
    "temp_df2[\"spent\"] = temp_df2[\"allocation\"] - temp_df2[\"remaining\"]\n",
    "\n",
    "# Variables defining under and appropriate spending ratio thresholds \n",
    "# TODO: CHECK AND MODIFY VALUES HERE\n",
    "                                                      \n",
    "under_spend_thres = 0.75\n",
    "par_spend_thres = 1.0\n",
    "temp_df2[\"under_spent\"] = temp_df2[\"spending_ratio\"] <= under_spend_thres\n",
    "temp_df2[\"over_spent\"] = temp_df2[\"spending_ratio\"] > par_spend_thres\n",
    "temp_df2[\"par_spent\"] = temp_df2[\"spending_ratio\"] == par_spend_thres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop outdated columns\n",
    "\n",
    "temp_df2 = temp_df2.drop(columns=[\"invoice_total\", \"funded_total\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Summary information for temp_df2\n",
    "\n",
    "temp_df2.info()\n",
    "temp_df2.head(100)\n",
    "temp_df2[\"under_spent\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot Encoding of temp_df2\n",
    "\n",
    "one_hot_df2 = temp_df2.set_index(\"membership_number\")\n",
    "\n",
    "# One-hot encoding of Gender\n",
    "\n",
    "one_hot_gender = pd.get_dummies(one_hot_df2[\"u_gender\"], prefix=\"gender\")\n",
    "\n",
    "# Merge with the cleaned DF2\n",
    "\n",
    "one_hot_df2 = pd.merge(one_hot_df2, one_hot_gender, on=\"membership_number\", how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding of GCCSA\n",
    "\n",
    "# Compile list of Greater regions\n",
    "\n",
    "greaterRegions = one_hot_df2[one_hot_df2[\"GCCSA_NAME\"].str.contains(\"Greater\", na=False)][\"GCCSA_NAME\"].value_counts().index.to_list()\n",
    "\n",
    "# Compile list of \"Rest of...\" regions\n",
    "\n",
    "restOfRegions = one_hot_df2[one_hot_df2[\"GCCSA_NAME\"].str.contains(\"Rest of\", na=False)][\"GCCSA_NAME\"].value_counts().index.to_list()\n",
    "\n",
    "# Compile list of \"Capital\" regions\n",
    "\n",
    "capitalRegions = one_hot_df2[one_hot_df2[\"GCCSA_NAME\"].str.contains(\"Capital\", na=False)][\"GCCSA_NAME\"].value_counts().index.to_list()\n",
    "\n",
    "# Replace \"Greater\" values with Urban\n",
    "\n",
    "for region in greaterRegions:\n",
    "    one_hot_df2 = one_hot_df2.replace([region], \"Urban\")\n",
    "\n",
    "# Replace \"Rest of...\" values with Rural\n",
    "\n",
    "for region in restOfRegions:\n",
    "    one_hot_df2 = one_hot_df2.replace([region], \"Rural\")\n",
    "    \n",
    "# Replace \"Capital\" regions with Urban\n",
    "\n",
    "for region in capitalRegions:\n",
    "    one_hot_df2 = one_hot_df2.replace([region], \"Urban\")\n",
    "    \n",
    "# Replace NaN values with \"Unknown\"\n",
    "\n",
    "one_hot_df2[\"GCCSA_NAME\"] = one_hot_df2[\"GCCSA_NAME\"].replace(np.nan, \"Unknown\")\n",
    "\n",
    "# Perform One-hot Encoding on GCCSA_NAME\n",
    "\n",
    "one_hot_region = pd.get_dummies(one_hot_df2[\"GCCSA_NAME\"], prefix=\"GCCSA\")\n",
    "one_hot_df2 = pd.merge(one_hot_df2, one_hot_region, on=\"membership_number\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert any UINT8 columns to Bool\n",
    "\n",
    "column_names = one_hot_df2.select_dtypes(include=[np.uint8]).columns\n",
    "one_hot_df2[column_names] = one_hot_df2[column_names].astype(bool)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "\n",
    "one_hot_df2 = one_hot_df2.drop(columns=[\"SA4\", \"SA4_NAME\", \"GCCSA_NAME\", \"STATE_NAME\", \"u_gender\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Summary information for one_hot_df2\n",
    "\n",
    "one_hot_df2.info()\n",
    "one_hot_df2.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual one-hot encode subtraction:\n",
    "\n",
    "df = one_hot_df2\n",
    "df['subtraction'] = df['subtraction'].astype(int)\n",
    "df['not_fully_funded'] = df['subtraction'].ge(500)\n",
    "df['acceptable_funded'] = df['subtraction'].lt(500)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One hot encode u_disabilities\n",
    "df = pd.get_dummies(df, prefix=['u_dis'], columns=['u_disabilities'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELING: CLUSTERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Pre processing for clustering\n",
    "df_og = df_for_cluster\n",
    "df_og.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for getting the avg of days from start to end\n",
    "df_og['start_date'] = pd.to_datetime(df_og['start_date'])\n",
    "df_og['funded_date'] = pd.to_datetime(df_og['funded_date'])\n",
    "df_og['days_between_start_funded'] = (df_og['funded_date'] - df_og['start_date']).dt.days\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Groupby member_id \n",
    "\n",
    "df_clus = df_og.groupby([\"membership_number\"]).agg({'days_between_start_funded': 'mean'}).reset_index()\n",
    "\n",
    "df_clus = df_clus.dropna(subset=['days_between_start_funded'])\n",
    "df_clus.info()\n",
    "df_clus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_df1 = pd.merge(temp_df2, df_clus, on=\"membership_number\", how=\"left\")\n",
    "final_df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#Mapping Regions\n",
    "# Compile list of Greater regions\n",
    "\n",
    "greaterRegions = final_df1[final_df1[\"GCCSA_NAME\"].str.contains(\"Greater\", na=False)][\"GCCSA_NAME\"].value_counts().index.to_list()\n",
    "\n",
    "# Compile list of \"Rest of...\" regions\n",
    "\n",
    "restOfRegions = final_df1[final_df1[\"GCCSA_NAME\"].str.contains(\"Rest of\", na=False)][\"GCCSA_NAME\"].value_counts().index.to_list()\n",
    "\n",
    "# Compile list of \"Capital\" regions\n",
    "\n",
    "capitalRegions = final_df1[final_df1[\"GCCSA_NAME\"].str.contains(\"Capital\", na=False)][\"GCCSA_NAME\"].value_counts().index.to_list()\n",
    "\n",
    "# Replace \"Greater\" values with Urban\n",
    "\n",
    "for region in greaterRegions:\n",
    "    final_df1 = final_df1.replace([region], \"Urban\")\n",
    "\n",
    "# Replace \"Rest of...\" values with Rural\n",
    "\n",
    "for region in restOfRegions:\n",
    "    final_df1 = final_df1.replace([region], \"Rural\")\n",
    "    \n",
    "# Replace \"Capital\" regions with Urban\n",
    "\n",
    "for region in capitalRegions:\n",
    "    final_df1 = final_df1.replace([region], \"Urban\")\n",
    "    \n",
    "# Replace NaN values with \"Unknown\"\n",
    "\n",
    "final_df1[\"GCCSA_NAME\"] = final_df1[\"GCCSA_NAME\"].replace(np.nan, \"Unknown\")\n",
    "\n",
    "final_df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing null values by mean\n",
    "final_df1['days_between_start_funded']= final_df1[\"days_between_start_funded\"].replace(np.nan, final_df1['days_between_start_funded'].mean())\n",
    "final_df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping unnecessary columns\n",
    "final_df1 = final_df1.drop(columns=[\"membership_number\", \"SA4\", \"SA4_NAME\", \"STATE_NAME\", \"member_key\", \"start_date\", \"end_date\", \"plan_key\", \"spending_ratio\" , \"spent\", \"under_spent\", \"over_spent\", \"par_spent\"])\n",
    "final_df1.info()\n",
    "#For association saving the dataframe\n",
    "final_df = final_df1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df1['GCCSA_NAME'].unique()\n",
    "#Maping the values of regions\n",
    "GCCSA_NAME_map = {\"Unknown\":3, \"Urban\": 1, \"Rural\": 2}\n",
    "final_df1['GCCSA_NAME'] = final_df1['GCCSA_NAME'].map(GCCSA_NAME_map)\n",
    "#final_df1.info()\n",
    "final_df1['GCCSA_NAME'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df1['u_gender'].unique()\n",
    "#Mapping\n",
    "u_gender_map = {\"Male\":1, \"Female\": 2, \"Other\": 3, \"Unknown\":4 }\n",
    "final_df1['u_gender'] = final_df1['u_gender'].map(u_gender_map)\n",
    "#final_df1.info()\n",
    "final_df1['u_gender'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df1['u_disabilities'].unique()\n",
    "#Mapping\n",
    "u_disabilities_map = {\"others\":1, \"Intelectual\": 2 }\n",
    "final_df1['u_disabilities'] = final_df1['u_disabilities'].map(u_disabilities_map)\n",
    "#final_df1.info()\n",
    "final_df1['u_disabilities'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final_df1.info()\n",
    "final_df1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert df to matrix\n",
    "X = final_df1.to_numpy()\n",
    "# scaling\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clustering k-prototypes\n",
    "rs= 42\n",
    "clusters = []\n",
    "cost_vals = []\n",
    "\n",
    "for k in range(2, 10, 2):\n",
    "    # train clustering with the specified K\n",
    "    model_clus = KPrototypes(n_clusters=k, random_state=rs, n_jobs=10)\n",
    "    model_clus.fit_predict(X, categorical=[1,2,3])\n",
    "    # append model to cluster list\n",
    "    clusters.append(model_clus)\n",
    "    cost_vals.append(model_clus.cost_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot the cost vs K values\n",
    "plt.plot(range(2,10,2), cost_vals, marker='*')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating Silhouette for K=4 K=6, K=8\n",
    "X_num = [[row[0], row[4], row[5],row[6]] for row in X] # Variables of X with numeric datatype\n",
    "X_cat = [[row[1], row[2], row[3]] for row in X] # variables of X with categorical datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just for reference. \n",
    "model = clusters[0] # cluster[0] holds the K-prtotypes model with K=2\n",
    "silScoreNums = silhouette_score(X_num, model.fit_predict(X,categorical=[1,2,3]), metric='euclidean')\n",
    "silScoreCats = silhouette_score(X_cat, model.fit_predict(X,categorical=[1,2,3]), metric='hamming')\n",
    "silScore = (silScoreNums + silScoreCats) / 2\n",
    "print(\"The avg Silhouette score for k=2: \" + str(silScore))\n",
    "\n",
    "model = clusters[1] # cluster[1] holds the K-prtotypes model with K=4\n",
    "\n",
    "# Calculate the Silhouette Score for the numeric and categorical variables seperately\n",
    "silScoreNums = silhouette_score(X_num, model.fit_predict(X,categorical=[1,2,3]), metric='euclidean')\n",
    "#print(\"Silscore for numeric variables: \" + str(silScoreNums))\n",
    "silScoreCats = silhouette_score(X_cat, model.fit_predict(X,categorical=[1,2,3]), metric='hamming') # note the metric here defined to `hamming`\n",
    "#print(\"Silscore for categorical variables: \" + str(silScoreCats))\n",
    "\n",
    "# Average the silhouette scores\n",
    "silScore = (silScoreNums + silScoreCats) / 2\n",
    "print(\"The avg silhouette score for k=4: \" + str(silScore))\n",
    "\n",
    "model = clusters[2]\n",
    "silScoreNums = silhouette_score(X_num, model.fit_predict(X,categorical=[1,2,3]), metric='euclidean')\n",
    "silScoreCats = silhouette_score(X_cat, model.fit_predict(X,categorical=[1,2,3]), metric='hamming')\n",
    "silScore = (silScoreNums + silScoreCats) / 2\n",
    "print(\"The avg Silhouette score for k=6: \" + str(silScore))\n",
    "\n",
    "model = clusters[3]\n",
    "silScoreNums = silhouette_score(X_num, model.fit_predict(X,categorical=[1,2,3]), metric='euclidean')\n",
    "silScoreCats = silhouette_score(X_cat, model.fit_predict(X,categorical=[1,2,3]), metric='hamming') \n",
    "silScore = (silScoreNums + silScoreCats) / 2\n",
    "print(\"The avg Silhouette score for k=8: \" + str(silScore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choosing K=2 from above and plotting pairplot. Can be modified (K=4/6/8) as needed.\n",
    "model = clusters[0] #cluster[0] is for K=2\n",
    "y=model.fit_predict(X, categorical=[1,2,3]) \n",
    "final_df1['Cluster_ID'] = y\n",
    "sns.color_palette(\"vlag\", as_cmap=True)\n",
    "# how many records are in each cluster\n",
    "print(\"Cluster membership\")\n",
    "print(final_df1['Cluster_ID'].value_counts())\n",
    "# pairplot the cluster distribution.\n",
    "cluster_g = sns.pairplot(final_df1, hue='Cluster_ID',diag_kind='hist',palette='Dark2')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choosing K=4 from above and plotting pairplot. Can be modified (K=4/6/8) as needed.\n",
    "model = clusters[1] \n",
    "y=model.fit_predict(X, categorical=[1,2,3]) \n",
    "final_df1['Cluster_ID'] = y\n",
    "sns.color_palette(\"vlag\", as_cmap=True)\n",
    "# how many records are in each cluster\n",
    "print(\"Cluster membership\")\n",
    "print(final_df1['Cluster_ID'].value_counts())\n",
    "# pairplot the cluster distribution.\n",
    "cluster_g = sns.pairplot(final_df1, hue='Cluster_ID',diag_kind='hist',palette='Dark2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Distribution for each cluster\n",
    "cols = ['subtraction', 'allocation', 'remaining', 'u_gender', 'GCCSA_NAME', 'u_disabilities', 'days_between_start_funded']\n",
    "n_bins = 20\n",
    "\n",
    "clusters_to_inspect = [0,1,2,3]\n",
    "\n",
    "for cluster in clusters_to_inspect:\n",
    "    print(\"Distribution for cluster {}\".format(cluster))\n",
    "    fig, ax = plt.subplots(nrows=7, figsize=(15,15))\n",
    "    ax[0].set_title(\"Cluster {}\".format(cluster))\n",
    "\n",
    "    for j, col in enumerate(cols):\n",
    "        bins = np.linspace(min(final_df1[col]), max(final_df1[col]), 20)\n",
    "        sns.distplot(final_df1[final_df1['Cluster_ID'] == cluster][col], bins=bins, ax=ax[j], norm_hist=True, kde_kws={'bw':1.5})\n",
    "        sns.distplot(final_df1[col], bins=bins, ax=ax[j], hist=False, color=\"k\")\n",
    "        \n",
    "    plt.subplots_adjust(bottom=0.1, right=1.8, top=4.5, wspace=0.8, hspace=0.8)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Association Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#using the dataframe of clustering before the mapping\n",
    "\n",
    "#final_df.info()\n",
    "#final_df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### All Claim details\n",
    "\n",
    "df_all_claims = pd.read_sql(\"select c.id as claim_id, invoice_id as invoiceId, c.item_category_level3_id from HH_claim c\", con=conn)\n",
    "\n",
    "#print(df_all_claims.info())\n",
    "#print(df_all_claims)\n",
    "### All item_categories details.\n",
    "\n",
    "##  \n",
    "df_all_ndis_service_cat = pd.read_sql(\"select item_category_level3_id, ndis.registration_group from hedgehog_ndis_service_item_ref ndis\", con=conn)\n",
    "#print(df_all_ndis_service_cat.info())\n",
    "#print(df_all_ndis_service_cat)\n",
    "\n",
    "##Combine df_all_claims and df_all_ndis_service_cat\n",
    "df_claimsWithProvider_details = pd.merge(df_all_claims, df_all_ndis_service_cat, on=\"item_category_level3_id\", how= 'inner')\n",
    "df_claimsWithProvider_details= df_claimsWithProvider_details.drop(columns=['item_category_level3_id'])\n",
    "#df_claimsWithProvider_details = pd.read_sql(\"select c.id, c.invoice_id as invoiceId, n.registration_group from HH_claim c left join hedgehog_ndis_service_item_ref n on c.item_category_level3_id = n.item_category_level3_id;\", con=conn)\n",
    "#print(df_claimsWithProvider_details.info())\n",
    "#print(df_claimsWithProvider_details)\n",
    "\n",
    "### Provider Services details (Provider account linked to invoice and HH_provider) took 1 minute to execute\n",
    "##  \n",
    "df_providersWithInvoice_details = pd.read_sql(\"select i.id as invoiceId, i.member_id from HH_invoice i;\", con=conn)\n",
    "#print(df_providersWithInvoice_details.info())\n",
    "#print(df_providersWithInvoice_details)\n",
    "\n",
    "##Combine df_claim_providers_details and df_providersWithInvoice_details\n",
    "df_claims_provider_details= pd.merge(df_claimsWithProvider_details, df_providersWithInvoice_details, on=\"invoiceId\", how=\"left\")\n",
    "\n",
    "##################################################\n",
    "\n",
    "#Final claim and Provider df\n",
    "print(df_claims_provider_details.info())\n",
    "print(df_claims_provider_details)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_u = pd.read_sql(\"select id as member_id, membership_number from HH_member m;\", con=conn)\n",
    "df_u.info()\n",
    "df_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#merge \n",
    "final_df_a = pd.merge(df_claims_provider_details, df_u, on= \"member_id\", how=\"left\")\n",
    "final_df_a= final_df_a.drop(columns=['claim_id', 'invoiceId', 'member_id'])\n",
    "final_df_a.info()\n",
    "final_df_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Prep for Association model\n",
    "# group by member num, then list all registration group\n",
    "services = final_df_a.groupby(['membership_number'])['registration_group'].apply(list)\n",
    "\n",
    "print(services.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " '''(1) If you are interested in generating associations that involve fairly rare services,\n",
    " you should consider reducing min_support. (2) If the items present in the dataset do not show high support, \n",
    " 'min_support' threshold should be set to small value and vice-versa. (3) If you obtain too many rules to be practically \n",
    " useful, you should consider increasing min_suport and min_confidence as a possible solution'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# type cast the services from pandas into normal list format and run apriori\n",
    "services_list = list(services)\n",
    "results = list(apriori(services_list, min_support=0.05))\n",
    "\n",
    "# print first 5 rules\n",
    "print(results[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_apriori_results_to_pandas_df(results):\n",
    "    rules = []\n",
    "    \n",
    "    for rule_set in results:\n",
    "        for rule in rule_set.ordered_statistics:\n",
    "            # items_base = left side of rules, items_add = right side\n",
    "            # support, confidence and lift for respective rules\n",
    "            rules.append([','.join(rule.items_base), ','.join(rule.items_add),\n",
    "                         rule_set.support, rule.confidence, rule.lift]) \n",
    "    \n",
    "    # typecast it to pandas df\n",
    "    return pd.DataFrame(rules, columns=['Left_side', 'Right_side', 'Support', \n",
    "                                        'Confidence', 'Lift']) \n",
    "\n",
    "result_df = convert_apriori_results_to_pandas_df(results)\n",
    "\n",
    "print(result_df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sort all acquired rules descending by lift\n",
    "result_df = result_df.sort_values(by='Lift', ascending=False)\n",
    "print(result_df.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODELING: NEURAL NETWORK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Drop columns with unique values.\n",
    "df = df.drop(columns=['spending_ratio',\"plan_key\", \"member_key\",\"start_date\", \"end_date\", 'subtraction', 'allocation', 'remaining', 'spent'])\n",
    "# Summary information for df\n",
    "df.info()\n",
    "df.head(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out model persistence where the trained model's weight was stored and loaded\n",
    "# To uncomment this code if you wish to train the model again or any changes subjected to the dataset\n",
    "'''\n",
    "# Specify the target column for analysis\n",
    "target_column = \"under_spent\"\n",
    "# Create the input df\n",
    "input_df = df.drop([target_column], axis=1)\n",
    "# Create a target df\n",
    "target_df = df[target_column]\n",
    "# Set random state\n",
    "random_state = 10\n",
    "# Set test size\n",
    "test_size = 0.3\n",
    "# Nump-ify input_df2\n",
    "input_df_mat = input_df.to_numpy()\n",
    "# Split training and test data\n",
    "input_df_train, input_df_test, target_df_train, target_df_test = train_test_split(input_df_mat, target_df, test_size=test_size, stratify=target_df, random_state=random_state)\n",
    "# Get standard scaler\n",
    "scaler = StandardScaler()\n",
    "# Transform training and test data\n",
    "input_df_train = scaler.fit_transform(input_df_train, target_df_train)\n",
    "input_df_test = scaler.transform(input_df_test)\n",
    "# Generate a prediction\n",
    "model_1 = MLPClassifier(random_state=random_state)\n",
    "model_1.fit(input_df_train, target_df_train)\n",
    "target_prediction = model_1.predict(input_df_test)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary Information\n",
    "print(\"Classification Report: \\n\", classification_report(target_df_test, target_prediction))\n",
    "print(\"Train Accuracy: \", model_1.score(input_df_train, target_df_train))\n",
    "print(\"Test Accuracy: \", model_1.score(input_df_test, target_df_test))\n",
    "print(\"Default Model Characteristics: \", model_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Improved Model - Neural Network tuned with GridSearchCV\n",
    "hiddenLayerSizes = [(2,), (3, ), (4, ), (5, ), (6, ), (7, )]\n",
    "\n",
    "alpha = [0.01, 0.001, 0.0001, 0.00001]\n",
    "\n",
    "params = {'hidden_layer_sizes': hiddenLayerSizes, 'alpha': alpha}\n",
    "\n",
    "model_2 = GridSearchCV(param_grid=params, estimator=MLPClassifier(random_state=random_state), cv=10, n_jobs=-1)\n",
    "\n",
    "model_2.fit(input_df_train, target_df_train)\n",
    "\n",
    "target_prediction = model_2.predict(input_df_test)\n",
    "# Summary Information\n",
    "print(\"Classification Report: \\n\", classification_report(target_df_test, target_prediction))\n",
    "print(\"Train Accuracy: \", model_2.score(input_df_train, target_df_train))\n",
    "print(\"Test Accuracy: \", model_2.score(input_df_test, target_df_test))\n",
    "print(\"Tuned Model Characteristics: \", model_2)\n",
    "print(\"Best Parameters: \\n\", model_2.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL PERSISTENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "#Save model weight:\n",
    "\n",
    "filename = 'finalized_model.sav'\n",
    "pickle.dump(model_1, open(filename, 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some time later...\n",
    " \n",
    "# load the model from disk\n",
    "\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "result = loaded_model.score(input_df_test, target_df_test)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "array = [[33,2,0,0,0,0,0,0,0,1,3], \n",
    "        [3,31,0,0,0,0,0,0,0,0,0], \n",
    "        [0,4,41,0,0,0,0,0,0,0,1], \n",
    "        [0,1,0,30,0,6,0,0,0,0,1], \n",
    "        [0,0,0,0,38,10,0,0,0,0,0], \n",
    "        [0,0,0,3,1,39,0,0,0,0,4], \n",
    "        [0,2,2,0,4,1,31,0,0,0,2],\n",
    "        [0,1,0,0,0,0,0,36,0,2,0], \n",
    "        [0,0,0,0,0,0,1,5,37,5,1], \n",
    "        [3,0,0,0,0,0,0,0,0,39,0], \n",
    "        [0,0,0,0,0,0,0,0,0,0,38]]\n",
    "df_cm = pd.DataFrame(array, index = [i for i in \"ABCDEFGHIJK\"],\n",
    "                  columns = [i for i in \"ABCDEFGHIJK\"])\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.heatmap(df_cm, annot=True)\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "04599353301972a4177495fb87ba9328f666fdd8bb33e7641e5bb86b309132ff"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "metadata": {
   "interpreter": {
    "hash": "04599353301972a4177495fb87ba9328f666fdd8bb33e7641e5bb86b309132ff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}